---
title: "Intro to torch"
author: "JMG"
format: html
---

```{r}
#| include: false
#| message: false

library(tidyverse)
library(tidymodels)
```

## Overview

### In `torch` world

`torch` is an R port of [PyTorch](https://pytorch.org/), one of the two most-employed deep learning frameworks in industry and research. The other is [TensorFlow](https://www.tensorflow.org/). The `torch` package is written entirely in R and C++ (including a bit of C). No Python installation is required to use it. Part of the goal of `torch` and similar libraries or packages is to implement fast and efficient numerical computations and to facilitate GPU computing. 

On the Python (PyTorch) side, the ecosystem appears as a set of concentric circles. In the middle, there's PyTorch itself, the core library without which nothing could work. Surrounding it, we have the inner circle of what could be called framework libraries, dedicated to special types of data (images, sound, text ...), or centered on workflow tasks, like deployment. Then, there is the broader ecosystem of add-ons, specializations, and libraries for whom PyTorch is a building block, or a tool. On the R side, we have the same "heart" -- all depends on core `torch` -- and we do have the same types of libraries; but the categories, the "circles", appear less clearly set off from each other. 

There are also three other related packages: `torchvision` , `torchaudio`, and `luz`. The former two bundle domain-specific transformations, deep learning models, datasets, and utilities for images (incl. video) and audio data, respectively. The third is a high-level, intuitive, nice-to-use interface to `torch`, allowing to define, train, and evaluate a neural network in just a few lines of code. Like `torch` itself, all three packages can be installed from CRAN.

### Installing and running torch

`torch` is available for Windows, MacOS, and Linux. If you have a compatible GPU, and the necessary NVidia software installed, you can benefit from significant speedup, a speedup that will depend on the type of model trained. At any time, you'll find up-to-date information in the [vignette](https://cran.r-project.org/web/packages/torch/vignettes/installation.html.

## Tensors

To do anything useful with `torch`, you need to know about tensors. Basically, a tensor is a container for data that can be of arbitrary dimension and that is optimized for fast computation on a GPU. In `torch`, tensors are the basic building blocks of neural networks.

Let's load torch and create a tensor.

```{r}
library(torch)
```


```{r}
t1 <- torch_tensor(c(0.1,1.2,6.5))
```

Now, `t1` has some attributes that we can inspect.

```{r}
(class(t1))
(t1$dtype)
(t1$device)
(t1$shape)
```

We can reshape our tensor.

```{r}
t2 <- torch_reshape(t1, c(3,1))
t3 <- torch_reshape(t1, c(1,3))
```


```{r}
t2$shape
```


```{r}
t3$shape
```


We won't go through the details here but there are a number of ways to create tensors of various sizes and data types. One thing we are specifically interested in though is how to get tensors from data sets in R. The trickiest part is handling data that is not numerical. We first have to covert everything to numerical there are no tensors in `torch` that store strings, then we need to construct an R matrix before we can convert it to a tensor.

```{r}
penguins_tt <- penguins %>%
  recipe(species ~ .) %>%
  step_naomit(everything()) %>%
  step_normalize(all_numeric()) %>%
  step_dummy(all_factor()) %>%
  prep() %>%
  juice() %>%
  as.matrix() %>%
  torch_tensor()
```


```{r}
penguins_tt
```


```{r}
(penguins_tt$shape)
(penguins_tt$device)
```


### Operations on Tensors


We can perform all the usual mathematical operations on tensors: add, subtract, divide ... These operations are available as functions (starting with `torch_`) as well as as methods on objects (invoked with `$`-syntax). For example, the following are equivalent:

```{r}
t1 <- torch_tensor(c(1, 2))
t2 <- torch_tensor(c(3, 4))

torch_add(t1, t2)
```

```{r}
# equivalently
t1$add(t2)
```


In both cases, a new object is created; neither `t1` nor `t2` are modified. There exists an alternate method that modifies its object in-place:

```{r}
t1$add_(t2)
```


```{r}
t1
```


In fact, the same pattern applies for other operations: Whenever you see an underscore appended, the object is modified in-place.

Naturally, in a scientific-computing setting, matrix operations are of special interest. Let's start with the dot product of two one-dimensional structures, i.e., vectors.

```{r}
t1 <- torch_tensor(1:3)
t2 <- torch_tensor(4:6)
t1$dot(t2)
```



Were you thinking this shouldn't work? Should we have needed to transpose (`torch_t()`) one of the tensors? In fact, this also works:

```{r}
t1$t()$dot(t2)
```

  

The reason the first call worked, too, is that `torch` does not distinguish between row vectors and column vectors. In consequence, if we multiply a vector with a matrix, using `torch_matmul()`, we don't need to worry about the vector's orientation either:

```{r}
t3 <- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))
t3$matmul(t1)
```

  
The same function, `torch_matmul()`, would be used to multiply two matrices. Note how this is different from what `torch_multiply()` does, namely, scalar-multiply its arguments:

```{r}
torch_multiply(t1, t2)
```


Many more tensor operations exist, some of which we will need to build neural networks. But there is one group that deserves special mention.

### Summary operations

If you have an R matrix and are about to compute a sum, this could, normally, mean one of three things: the global sum, row sums, or column sums. Let's see all three of them at work (using `apply()` for a reason):

```{r}
(m <- outer(1:3, 1:6))

sum(m)
apply(m, 1, sum)
apply(m, 2, sum)
```


And now, the `torch` equivalents. We start with the overall sum.

```{r}
t <- torch_outer(torch_tensor(1:3), torch_tensor(1:6))
t$sum()
```


It gets more interesting for the row and column sums. The `dim` argument tells `torch` which dimension(s) to sum over. Passing in `dim = 1`, we see:

```{r}
t$sum(dim = 1)
```


Unexpectedly, these are the column sums! Before drawing conclusions, let's check what happens with `dim = 2`:

```{r}
t$sum(dim = 2)
```


Now, we have sums over rows. Did we misunderstand something about how `torch` orders dimensions? No, it's not that. In `torch`, when we're in two dimensions, we think rows first, columns second. (And as you'll see in a minute, we start indexing with 1, just as in R in general.)

Instead, the conceptual difference is specific to aggregating, or "grouping", operations. In R, *grouping*, in fact, nicely characterizes what we have in mind: We group by row (dimension 1) for row summaries, by column (dimension 2) for column summaries. In `torch`, the thinking is different: We *collapse* the columns (dimension 2) to compute row summaries, the rows (dimension 1) for column summaries.

The same thinking applies in higher dimensions. Assume, for example, that we been recording time series data for four individuals. There are two features, and both of them have been measured at three times. If we were planning to train a recurrent neural network (much more on that later), we would arrange the measurements like so:

-   Dimension 1: Runs over individuals.

-   Dimension 2: Runs over points in time.

-   Dimension 3: Runs over features.

The tensor then would look like this:

```{r}
t <- torch_randn(4, 3, 2)
t
```


To obtain feature averages, independently of subject and time, we would collapse dimensions 1 and 2:

```{r}
t$mean(dim = c(1, 2))
```


If, on the other hand, we wanted feature averages, but individually per person, we'd do:

```{r}
t$mean(dim = 2)
```


Here, the single feature "collapsed" is the time step.

## Automatic Differentiation

The key to training neural networks is the ability to compute gradients. In `torch`, this is done automatically, using the `autograd` functionality which implements automatic differentiation. The basic idea is that we define a function, and `autograd` computes its gradient. The function can be arbitrary, but it must be differentiable. Let's explore how this works in `torch`.

In supervised machine learning, we have at our disposal a *training set*, where the variable we're hoping to predict is known. This is the target, or *ground truth*. We now develop and train a prediction algorithm, based on a set of input variables, the *predictors*. This training, or learning, process, is based on comparing the algorithm's predictions with the ground truth, a comparison that leads to a number capturing how good or bad the current predictions are. To provide this number is the job of the *loss function*.

Given the current loss, an algorithm can adjust its parameters -- for example, the *weights* in a neural network -- in order to deliver better predictions. It just has to know in which direction to adjust them. This information is made available by the *gradient*.

As an example, we imagine a loss function that looks like (@fig-autograd-paraboloid):

![Hypothetical loss function (a paraboloid).](https://github.com/skeydan/Deep-Learning-and-Scientific-Computing-with-R-torch/blob/main/images/autograd-paraboloid.png?raw=true){#fig-autograd-paraboloid fig-alt="A paraboloid in two dimensions that has a minimum at (0,0)."}

This is a quadratic function of two variables: $f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5$. It has its minimum at `(0,0)`, and this is the point we'd like to be at. As humans, standing at the location designated by the white dot, and looking at the landscape, we have a pretty clear idea how to go downhill quickly. To find the best direction computationally, however, we compute the gradient.

Take the $x_1$ direction. The derivative of the function with respect to $x_1$ indicates how its value varies as $x_1$ varies. As we know the function in closed form, we can compute that: $\frac{\partial f}{\partial x_1} = 0.4 x_1$. This tells us that as $x_1$ increases, loss increases, and how fast. But we want loss to *decrease*, so we have to go in the opposite direction.

The same holds for the $x_2$-axis. We compute the derivative ($\frac{\partial f}{\partial x_2} = 0.4 x_2$). Again, we want to take the direction opposite to where the derivative points. Overall, this yields a descent direction of $\begin{bmatrix}-0.4x_1\\-0.4x_2 \end{bmatrix}$.

Descriptively, this strategy is called *steepest descent*\index{descent!steepest}. Commonly referred to as *gradient descent*\index{descent!gradient}, it is the most basic optimization algorithm in deep learning. Perhaps unintuitively, it is not always the most efficient way. And there's another question: Can we assume that this direction, computed at the starting point, will remain optimal as we continue descending? Maybe we'd better regularly recompute directions instead? This is why we iterate with gradient descent, and why we need to compute gradients efficiently.

## Automatic differentiation example


@fig-autograd-compgraph is how our above function could be represented in a computational graph. `x1` and `x2` are input nodes, corresponding to function parameters $x_1$ and $x_2$. `x7` is the function's output; all other nodes are intermediate ones, necessary to ensure correct order of execution. (We could have given the constants, `-5` , `0.2`, and `2`, their own nodes as well; but as they're remaining, well, constant anyway, we're not too interested in them and prefer having a simpler graph.)

![Example of a computational graph.](https://github.com/skeydan/Deep-Learning-and-Scientific-Computing-with-R-torch/blob/main/images/autograd-compgraph.png?raw=true){#fig-autograd-compgraph fig-alt="A directed graph where nodes represent data, and arrows, mathematical operations. There are two input nodes, four intermediate nodes, and one output node. Operations used are exponentiation, multiplication, and addition."}

In reverse-mode AD, the flavor of automatic differentiation implemented by `torch`, the first thing that happens is to calculate the function's output value. This corresponds to a forward pass through the graph. Then, a backward pass is performed to calculate the gradient of the output with respect to both inputs, `x1` and `x2`. In this process, information becomes available, and is built up, from the right:

-   At `x7`, we calculate partial derivatives with respect to `x5` and `x6`. Basically, the equation to differentiate looks like this: $f(x_5, x_6) = x_5 + x_6 - 5$. Thus, both partial derivatives are 1.

-   From `x5`, we move to the left to see how it depends on `x3`. We find that $\frac{\partial x_5}{\partial x_3} = 0.2$. At this point, applying the chain rule of calculus, we already know how the output depends on `x3`: $\frac{\partial f}{\partial x_3} = 0.2 * 1 = 0.2$.

-   From `x3`, we take the final step to `x`. We learn that $\frac{\partial x_3}{\partial x_1} = 2 x_1$. Now, we again apply the chain rule, and are able to formulate how the function depends on its first input: $\frac{\partial f}{\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1$.

-   Analogously, we determine the second partial derivative, and thus, already have the gradient available: $\nabla f = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} = 0.4 x_1 + 0.4 x_2$.

That is the principle. In practice, different frameworks implement reverse-mode automatic differentiation differently. Let's take a look at how `torch` does it.

## Automatic differentiation with `torch` *autograd*

First, a quick note on terminology. In `torch`, the AD engine is usually referred to as *autograd*.

To construct the above computational graph with `torch`, we create "source" tensors `x1` and `x2`. These will mimic the parameters whose impact we're interested in. However, if we just proceed "as usual", creating the tensors the way we've been doing so far, `torch` will not prepare for AD. Instead, we need to pass in `requires_grad = TRUE` when instantiating those tensors:

```{r}

x1 <- torch_tensor(2, requires_grad = TRUE)
x2 <- torch_tensor(2, requires_grad = TRUE)
```

(By the way, the value `2` for both tensors was chosen completely arbitrarily.)

Now, to create "invisible" nodes `x3` to `x6` , we square and multiply accordingly. Then `x7` stores the final result.

```{r}
x3 <- x1$square()
x5 <- x3 * 0.2

x4 <- x2$square()
x6 <- x4 * 0.2

x7 <- x5 + x6 - 5
x7
```

  

Note that we have to add `requires_grad = TRUE` when creating the "source" tensors only. All dependent nodes in the graph inherit this property. For example:

```{r}
x7$requires_grad
```

    

Now, all prerequisites are fulfilled to see automatic differentiation at work. All we need to do to determine how `x7` depends on `x1` and `x2` is call `backward()`:

```{r}
x7$backward()
```

Due to this call, the `$grad` fields have been populated in `x1` and `x2`:

```{r}
x1$grad
x2$grad
```

     

These are the partial derivatives of `x7` with respect to `x1` and `x2`, respectively. Conforming to our manual calculations above, both amount to 0.8, that is, 0.4 times the tensor values 2 and 2.

How about the accumulation process we said was needed to build up those end-to-end derivatives? Can we "follow" the end-to-end derivative as it's being built up? For example, can we see how the final output depends on `x3`?

```{r}
x3$grad
```

    

The field does not seem to be populated. In fact, while it *has* to compute them, `torch` throws away the intermediate aggregates once they are no longer needed, to save memory. We can, however, ask it to keep them, using `retain_grad = TRUE`:

```{r}
x3 <- x1$square()
x3$retain_grad()

x5 <- x3 * 0.2
x5$retain_grad()

x4 <- x2$square()
x4$retain_grad()

x6 <- x4 * 0.2
x6$retain_grad()

x7 <- x5 + x6 - 5
x7$backward()
```

Now, we find that `x3`'s `grad` field *is* populated:

```{r}
x3$grad
```

    

The same goes for `x4`, `x5`, and `x6`:

```{r}
x4$grad
x5$grad
x6$grad
```

    

There is one remaining thing we might be curious about. We've managed to catch a glimpse of the gradient-accumulation process from the "running gradient" point of view, in a sense; but how about the individual derivatives that need to be taken in order to proceed with accumulation? For example, what `x3$grad` tells us is how the output depends on the intermediate state at `x3`; how do we get from there to `x1`, the actual input node?

It turns out that of that aspect, too, we can get an idea. During the forward pass, `torch` already takes a note on what it will have to do, later, to calculate the individual derivatives. This "recipe" is stored in a tensor's `grad_fn` field. For `x3`, this adds the "missing link" to `x1`:

```{r}
x3$grad_fn
```

    

The same works for `x4`, `x5`, and `x6`:

```{r}
x4$grad_fn
x5$grad_fn
x6$grad_fn
```

    

And there we are! We've seen how `torch` computes derivatives for us, and we've even caught a glimpse of how it does it. Now, we are ready to play around with our first two applied tasks.

### Function Minimization using Autograd via `torch`


```{r}

a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}

num_iterations <- 1000

lr <- 0.01

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")

  value <- rosenbrock(x)
  if (i %% 100 == 0) {
    cat("Value is: ", as.numeric(value), "\n")
  }

  value$backward()
  if (i %% 100 == 0) {
    cat("Gradient is: ", as.matrix(x$grad), "\n")
  }

  with_no_grad({
    x$sub_(lr * x$grad)
    x$grad$zero_()
  })
}

x
```






We can now code a simple neural network from scratch by implementing the following steps:

-   compute predictions (forward pass),

-   calculate the loss,

-   have *autograd* compute partial derivatives (calling `loss$backward()`), and

-   update the parameters, subtracting from each some fraction of the gradient.

In code:

```{r}
# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

# weights connecting input to hidden layer
w1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 <- torch_zeros(1, d_out, requires_grad = TRUE)

learning_rate <- 1e-4

### training loop ----------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  
  y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)
  
  ### -------- Compute loss -------- 
  loss <- (y_pred - y)$pow(2)$mean()
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with
  # requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we don't 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 <- w1$sub_(learning_rate * w1$grad)
     w2 <- w2$sub_(learning_rate * w2$grad)
     b1 <- b1$sub_(learning_rate * b1$grad)
     b2 <- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they'd
     # accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}

```



















